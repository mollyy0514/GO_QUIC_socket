{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"/Volumes/mollyT7/MOXA/\"\n",
    "# database = \"/home/wmnlab/Documents/r12921105\"\n",
    "date = \"20240411\"\n",
    "date_with_dash = \"2024-04-11\"\n",
    "port = \"5200\"\n",
    "phone_time = \"0629\"\n",
    "time = \"1428\"\n",
    "device = \"sm00\"\n",
    "if int(port) % 2 == 0:\n",
    "    sent_file_name = f\"log_{date}_{phone_time}_{port}_client\"\n",
    "    received_file_name = f\"log_{date}_{time}_{port}_server\"\n",
    "else:\n",
    "    sent_file_name = f\"log_{date}_{time}_{port}_server\"\n",
    "    received_file_name = f\"log_{date}_{phone_time}_{port}_client\"\n",
    "path = f\"{database}/{date_with_dash}/QUIC-inf/{device}/#01/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_file_name = f\"{database}/{date_with_dash}/time_sync_{device}.json\"\n",
    "# sync_file = path + \"raw/\" + sync_file_name\n",
    "with open(sync_file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract values from the dictionary\n",
    "values = list(data.values())\n",
    "mean_diff = values[0] * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to JSON & CSV file\n",
    "Process the qlog file to json file & csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QlogToJsonEntry(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Add commas between lines\n",
    "    json_str = \",\".join(lines)\n",
    "    # Surround the entire string with square brackets to create a JSON array\n",
    "    json_str = \"[\" + json_str + \"]\"\n",
    "    # Load the JSON array\n",
    "    json_entry = json.loads(json_str)\n",
    "    \n",
    "    return json_entry\n",
    "\n",
    "def QlogToJson(json_entry, json_file_path):\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(json_entry, json_file, indent=2)\n",
    "\n",
    "def JsonToCsv(json_entry, csv_file_path):\n",
    "     # Open CSV file for writing\n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        # Create a CSV writer\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write header row based on the keys of the second JSON object (assuming at least two objects are present)\n",
    "        if len(json_entry) >= 2:\n",
    "            header = list(json_entry[1].keys())\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "            # Write data rows starting from the second object\n",
    "            for entry in json_entry[1:]:\n",
    "                csv_writer.writerow(entry.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender_side_file\n",
    "sent_raw_path = path + \"raw/\" + sent_file_name\n",
    "sent_qlog_file_path = sent_raw_path + \".qlog\"\n",
    "sent_json_file_path = sent_raw_path + \".json\"\n",
    "sent_csv_file_path = sent_raw_path + \".csv\"\n",
    "sent_json_entry = QlogToJsonEntry(sent_qlog_file_path)\n",
    "# QlogToJson(sent_json_entry, sent_json_file_path)\n",
    "JsonToCsv(sent_json_entry, sent_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "received_raw_path = path + \"raw/\" + received_file_name\n",
    "received_qlog_file_path = received_raw_path + \".qlog\"\n",
    "received_json_file_path = received_raw_path + \".json\"\n",
    "received_csv_file_path = received_raw_path + \".csv\"\n",
    "received_json_entry = QlogToJsonEntry(received_qlog_file_path)\n",
    "# QlogToJson(received_json_entry, received_json_file_path)\n",
    "JsonToCsv(received_json_entry, received_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv(sent_csv_file_path)\n",
    "received_df = pd.read_csv(received_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set time to UMT+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStartTime(json_data):\n",
    "    # unit: ms\n",
    "    refTime = json_data[0][\"trace\"][\"common_fields\"][\"reference_time\"]\n",
    "    return refTime\n",
    "\n",
    "def ProcessTime(df, reference_time):\n",
    "    # Extract the \"time\" values from the DataFrame\n",
    "    original_times = (df['time'].astype(float))\n",
    "\n",
    "    # Calculate \"epoch_time\" and convert to timestamps\n",
    "    epoch_times = (reference_time + original_times)\n",
    "    timestamps = pd.to_datetime(epoch_times, unit='ms').dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    df['epoch_time'] = epoch_times\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No matter downlink or uplink, the file time that need to change is client side.\n",
    "if int(port)%2 == 0: # UL\n",
    "    clientStartTime = GetStartTime(sent_json_entry)\n",
    "    print(clientStartTime)\n",
    "    serverStartTime = GetStartTime(received_json_entry)\n",
    "    print(serverStartTime)\n",
    "\n",
    "    senderRefTime = clientStartTime + mean_diff\n",
    "    rcverRefTime = serverStartTime\n",
    "\n",
    "else:   # DL\n",
    "    clientStartTime = GetStartTime(received_json_entry)\n",
    "    print(clientStartTime)\n",
    "    serverStartTime = GetStartTime(sent_json_entry)\n",
    "    print(serverStartTime)\n",
    "    startTimeDiff = (clientStartTime - serverStartTime) + mean_diff\n",
    "\n",
    "    senderRefTime = serverStartTime\n",
    "    rcverRefTime = clientStartTime + mean_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = ProcessTime(sent_df, senderRefTime)\n",
    "# Add 8 hours to both epoch times and timestamps to match UMT+8\n",
    "# Also sync time with server\n",
    "epoch_times_gmt8 = sent_df[\"epoch_time\"] + 8 * 3600 * 1000\n",
    "sent_df[\"epoch_time\"] = epoch_times_gmt8\n",
    "timestamps_gmt8 = pd.to_datetime(epoch_times_gmt8, unit='ms').dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "sent_df[\"timestamp\"] = timestamps_gmt8\n",
    "\n",
    "sent_df[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "received_df = ProcessTime(received_df, rcverRefTime)\n",
    "# if the sender is server, then it is no need to calculate time difference\n",
    "epoch_times_gmt8 = received_df[\"epoch_time\"] + 8 * 3600 * 1000\n",
    "received_df[\"epoch_time\"] = epoch_times_gmt8\n",
    "timestamps_gmt8 = pd.to_datetime(epoch_times_gmt8, unit='ms').dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "received_df[\"timestamp\"] = timestamps_gmt8\n",
    "\n",
    "received_df[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add RealTime to CSV\n",
    "new_csv_order = ['time', 'epoch_time', 'timestamp', 'name', 'data']\n",
    "sent_df = sent_df[new_csv_order]\n",
    "received_df = received_df[new_csv_order]\n",
    "\n",
    "sent_df.to_csv(sent_csv_file_path, index=False)\n",
    "received_df.to_csv(received_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender side data\n",
    "metrics_all_rows = sent_df[(sent_df['name'] == 'recovery:metrics_updated') & (sent_df['data'].str.contains(\"'bytes_in_flight':\"))]\n",
    "metrics_sent_rows = sent_df[(sent_df['name'] == 'recovery:metrics_updated') & (sent_df['data'].str.contains(\"{'bytes_in_flight':\"))]\n",
    "metrics_ack_rows = sent_df[(sent_df['name'] == 'recovery:metrics_updated') & (sent_df['data'].str.contains(\"'latest_rtt':\"))]\n",
    "total_sent_rows = sent_df[(sent_df['name'] == 'transport:packet_sent')]\n",
    "pk_sent_rows = sent_df[(sent_df['name'] == 'transport:packet_sent') & (sent_df['data'].str.contains(\"'frame_type': 'stream'\"))]\n",
    "rcv_ack_rows = sent_df[(sent_df['name'] == 'transport:packet_received') & (sent_df['data'].str.contains(\"'frame_type': 'ack'\")) & (sent_df['data'].str.contains(\"'packet_type': '1RTT'\"))]\n",
    "lost_rows = sent_df[sent_df['name'] == 'recovery:packet_lost']\n",
    "\n",
    "# Get the count of rows\n",
    "metrics_all_cnt = len(metrics_all_rows)\n",
    "metrics_c_cnt = len(metrics_sent_rows)\n",
    "metrics_ack_cnt = len(metrics_ack_rows)\n",
    "total_sent_cnt = len(total_sent_rows)\n",
    "pk_sent_cnt = len(pk_sent_rows)\n",
    "rcv_ack_cnt = len(rcv_ack_rows)\n",
    "lost_cnt = len(lost_rows)\n",
    "\n",
    "print(\"packet_sent: \", pk_sent_cnt, metrics_c_cnt)\n",
    "print(\"ack: \", rcv_ack_cnt, metrics_ack_cnt)\n",
    "print(metrics_all_cnt, metrics_c_cnt, metrics_ack_cnt, pk_sent_cnt, rcv_ack_cnt, lost_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_rcv_rows = received_df[(received_df['name'] == \"transport:packet_received\") & (received_df['data'].str.contains(\"'frame_type': 'stream'\"))]\n",
    "pk_rcv_rows = pk_rcv_rows.reset_index(drop=True)\n",
    "print(len(pk_rcv_rows))\n",
    "pk_rcv_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with sender side data\n",
    "Concat `transport:packet_sent` & `recovery:metrics_updated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sent_csv_file_path = path + \"middle/\" + f\"sent_metrics_{time}_{port}.csv\"\n",
    "metrics_sent_rows.to_csv(metrics_sent_csv_file_path, index=False)\n",
    "pk_sent_csv_file_path = path + \"middle/\" + f\"pk_sent_{time}_{port}.csv\"\n",
    "pk_sent_rows.to_csv(pk_sent_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(df, idx, new_row):\n",
    "    df1 = df.iloc[:idx, :]\n",
    "    df2 = df.iloc[idx:, :]\n",
    "    df_new = pd.concat([df1, new_row, df2], ignore_index=True)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sent_rows = metrics_sent_rows.reset_index(drop=True)\n",
    "pk_sent_rows = pk_sent_rows.reset_index(drop=True)\n",
    "print(metrics_sent_rows[:5])\n",
    "print(pk_sent_rows[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_recover_c_len = len(metrics_sent_rows)\n",
    "for i in range(pk_sent_cnt):\n",
    "    if(i >= len(metrics_sent_rows)):\n",
    "        data = metrics_sent_rows.iloc[i-1]['data']\n",
    "        new_row_data = {'time': [pk_sent_rows.iloc[i]['time']], 'name':['recovery:metrics_updated'], 'data': [data]}\n",
    "        new_row = pd.DataFrame(new_row_data)\n",
    "        metrics_sent_rows = pd.concat([metrics_sent_rows, new_row], ignore_index=True)\n",
    "        continue\n",
    "    time_diff = metrics_sent_rows.iloc[i]['time'] - pk_sent_rows.iloc[i]['time']\n",
    "    # print(i, time_diff)\n",
    "    # time_diff >= 1: not the matching metrics_update\n",
    "    while time_diff >= 1:\n",
    "        data = metrics_sent_rows.iloc[i-1]['data']\n",
    "        new_row_data = {'time': [pk_sent_rows.iloc[i]['time']], 'name':['recovery:metrics_updated'], 'data': [data]}\n",
    "        new_row = pd.DataFrame(new_row_data)\n",
    "        # print(new_row)\n",
    "        metrics_sent_rows = insert(metrics_sent_rows, i, new_row)\n",
    "        time_diff = metrics_sent_rows.iloc[i]['time'] - pk_sent_rows.iloc[i]['time']\n",
    "    # time_diff < 0: missing metrics_update\n",
    "    while time_diff < 0:\n",
    "        # print(i, time_diff_list)\n",
    "        metrics_sent_rows.drop(index=metrics_sent_rows.index[i], inplace=True)\n",
    "        time_diff = metrics_sent_rows.iloc[i]['time'] - pk_sent_rows.iloc[i]['time']\n",
    "\n",
    "    \n",
    "\n",
    "# if len(metrics_sent_rows) < pk_sent_cnt:\n",
    "#     d = pk_sent_cnt - len(metrics_sent_rows)\n",
    "# data = metrics_sent_rows.iloc[len(metrics_sent_rows)-1]['data']\n",
    "\n",
    "# for i in range(d):\n",
    "#     last_row_data = {'time': [pk_sent_rows.iloc[len(metrics_sent_rows)-1]['time']], 'name':['recovery:metrics_updated'], 'data': [data]}\n",
    "#     new_row_df = pd.DataFrame(last_row_data)\n",
    "#     metrics_sent_rows = pd.concat([metrics_sent_rows, new_row], ignore_index=True)\n",
    "\n",
    "print(ori_recover_c_len, len(metrics_sent_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sent_rows = metrics_sent_rows.reset_index(drop=True)\n",
    "pk_sent_rows = pk_sent_rows.reset_index(drop=True)\n",
    "print(len(metrics_sent_rows), len(pk_sent_rows))\n",
    "\n",
    "# check whether there's still mismatch exist.\n",
    "time_diff_list = metrics_sent_rows['time'] - pk_sent_rows['time']\n",
    "mismatch_indices = time_diff_list[(time_diff_list >= 1) | (time_diff_list < 0)].index\n",
    "if len(mismatch_indices) == 0:\n",
    "    print(\"All Matched!\")\n",
    "else:\n",
    "    print(mismatch_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bytes_in_flight & packets_in_flight\n",
    "metrics_sent_rows['bytes_in_flight'] = None\n",
    "metrics_sent_rows['packets_in_flight'] = None\n",
    "\n",
    "# Use ast.literal_eval to safely evaluate the string and extract 'bytes_in_flight' and 'packets_in_flight'\n",
    "metrics_sent_rows[['bytes_in_flight', 'packets_in_flight']] = metrics_sent_rows['data'].apply(\n",
    "    lambda x: pd.Series(ast.literal_eval(x)) if isinstance(x, str) else pd.Series([None, None]))\n",
    "\n",
    "metrics_sent_rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bytes_in_flight & packets_in_flight to pk_sent_rows\n",
    "pk_sent_rows['bytes_in_flight'] = metrics_sent_rows['bytes_in_flight']\n",
    "pk_sent_rows['packets_in_flight'] = metrics_sent_rows['packets_in_flight']\n",
    "\n",
    "pk_sent_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat `transport:packet_received` & `recovery:metrics_updated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ack_csv_file_path = path + \"middle/\" + f\"ack_metrics_{time}_{port}.csv\" \n",
    "metrics_ack_rows.to_csv(metrics_ack_csv_file_path, index=False)\n",
    "rcv_ack_csv_file_path = path + \"middle/\" + f\"rcv_ack_{time}_{port}.csv\"\n",
    "rcv_ack_rows.to_csv(rcv_ack_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ack_rows = metrics_ack_rows.reset_index(drop=True)\n",
    "rcv_ack_rows = rcv_ack_rows.reset_index(drop=True)\n",
    "initial_ack_metrics = metrics_ack_rows.iloc[[0]]\n",
    "metrics_ack_rows.drop(index=metrics_ack_rows.index[0], inplace=True)\n",
    "metrics_ack_rows = metrics_ack_rows.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ack_rows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv_ack_rows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(metrics_ack_rows), len(rcv_ack_rows))\n",
    "for i in range(rcv_ack_cnt):\n",
    "    if(i >= len(metrics_ack_rows)):\n",
    "        data = metrics_ack_rows.iloc[i-1]['data']\n",
    "        new_row_data = {'time': [rcv_ack_rows.iloc[i]['time']], 'name':['recovery:metrics_updated'], 'data': [data]}\n",
    "        new_row = pd.DataFrame(new_row_data)\n",
    "        metrics_ack_rows = pd.concat([metrics_ack_rows, new_row], ignore_index=True)\n",
    "        continue\n",
    "    time_diff = metrics_ack_rows.iloc[i]['time'] - rcv_ack_rows.iloc[i]['time']\n",
    "    # time_diff >= 1: not the matching metrics_update\n",
    "    while time_diff > 0:\n",
    "        # print(\"> 0:\", i, time_diff)\n",
    "        if i == 0:\n",
    "            data = initial_ack_metrics.iloc[0]['data']\n",
    "        else:\n",
    "            data = metrics_ack_rows.iloc[i-1]['data']\n",
    "        new_row_data = {'time': [rcv_ack_rows.iloc[i]['time']], 'name':['recovery:metrics_updated'], 'data': [data]}\n",
    "        new_row = pd.DataFrame(new_row_data)\n",
    "        metrics_ack_rows = insert(metrics_ack_rows, i, new_row)\n",
    "        time_diff = metrics_ack_rows.iloc[i]['time'] - rcv_ack_rows.iloc[i]['time']\n",
    "    # time_diff < 0: missing metrics_update\n",
    "    while time_diff <= -1:\n",
    "        # print(\"<= -1:\", i, time_diff)\n",
    "        metrics_ack_rows.drop(index=metrics_ack_rows.index[i], inplace=True)\n",
    "        time_diff = metrics_ack_rows.iloc[i]['time'] - rcv_ack_rows.iloc[i]['time']\n",
    "print(len(metrics_ack_rows), len(rcv_ack_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ack_rows = metrics_ack_rows.reset_index(drop=True)\n",
    "rcv_ack_rows = rcv_ack_rows.reset_index(drop=True)\n",
    "\n",
    "# check whether there's still mismatch exist.\n",
    "time_diff_list = metrics_ack_rows['time'] - rcv_ack_rows['time']\n",
    "mismatch_indices = time_diff_list[(time_diff_list <= -1) | (time_diff_list > 0)].index\n",
    "if len(mismatch_indices) == 0:\n",
    "    print(\"All Matched!\")\n",
    "else:\n",
    "    print(mismatch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ack_json_list = []\n",
    "## Add the initial_ack_metrics for temporary\n",
    "print(initial_ack_metrics)\n",
    "metrics_ack_rows = pd.concat([initial_ack_metrics, metrics_ack_rows], axis=0).reset_index(drop=True)\n",
    "for i in range(len(metrics_ack_rows)):\n",
    "    s = metrics_ack_rows.iloc[i]['data'].replace(\"\\'\", \"\\\"\")\n",
    "    json_object = json.loads(s)\n",
    "    ack_json_list.append(json_object)\n",
    "\n",
    "metrics_ack_df = pd.DataFrame(ack_json_list)\n",
    "# Fill missing values in each row with the previous row's values\n",
    "metrics_ack_df = metrics_ack_df.ffill(axis=0)\n",
    "\n",
    "## drop initial_ack_metrics\n",
    "metrics_ack_rows.drop(index=metrics_ack_rows.index[0], inplace=True)\n",
    "metrics_ack_rows = metrics_ack_rows.reset_index(drop=True)\n",
    "metrics_ack_df.drop(index=metrics_ack_df.index[0], inplace=True)\n",
    "metrics_ack_df = metrics_ack_df.reset_index(drop=True)\n",
    "metrics_ack_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ack_rows = pd.concat([metrics_ack_rows, metrics_ack_df], axis=1).reset_index(drop=True)\n",
    "# since we have parse out all the information in data, we can drop the data cl=olumn\n",
    "metrics_ack_rows = metrics_ack_rows.drop(columns=['data'])\n",
    "metrics_ack_rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whehter the length is equal before concating metrics into rcv_ack_rows\n",
    "print(len(rcv_ack_rows), len(metrics_ack_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv_ack_rows = pd.concat([rcv_ack_rows, metrics_ack_df], axis=1)\n",
    "rcv_ack_rows = rcv_ack_rows.reset_index(drop=True)\n",
    "\n",
    "print(len(rcv_ack_rows), len(metrics_ack_df))\n",
    "rcv_ack_rows[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the ACK ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acked_ranges_series = rcv_ack_rows['data']\n",
    "acked_ranges_list = []\n",
    "for i in range(len(acked_ranges_series)):\n",
    "    s = acked_ranges_series.iloc[i]\n",
    "    data_dict = json.loads(s.replace(\"\\'\", \"\\\"\"))\n",
    "    # Extract 'acked_ranges' from all frames\n",
    "    acked_ranges = [range_entry for frame in data_dict['frames'] if 'acked_ranges' in frame for range_entry in frame['acked_ranges']]\n",
    "    acked_ranges_list.append(acked_ranges)\n",
    "\n",
    "acked_ranges_df = pd.DataFrame({\"acked_ranges\": acked_ranges_list})\n",
    "acked_ranges_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv_ack_rows = pd.concat([rcv_ack_rows, acked_ranges_df], axis=1)\n",
    "rcv_ack_rows = rcv_ack_rows.reset_index(drop=True)\n",
    "\n",
    "rcv_ack_rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out the packet_number & offset & length\n",
    "pk_sent_series =  pk_sent_rows['data']\n",
    "pk_num_list = []\n",
    "offset_list = []\n",
    "length_list = []\n",
    "for i in range(len(pk_sent_series)):\n",
    "    s = pk_sent_series.iloc[i]\n",
    "    data_dict = json.loads(s.replace(\"\\'\", \"\\\"\"))\n",
    "    packet_number = data_dict['header']['packet_number']\n",
    "    # Initialize offset to None in case 'frame_type': 'stream' is not found\n",
    "    offset = None\n",
    "    # Iterate through frames to find 'offset' for 'frame_type': 'stream'\n",
    "    for frame in data_dict.get('frames', []):\n",
    "        if frame.get('frame_type') == 'stream':\n",
    "            offset = frame.get('offset')\n",
    "            length = frame.get('length')\n",
    "            break  # Stop iterating once 'offset' is found\n",
    "    \n",
    "    pk_num_list.append(packet_number)\n",
    "    offset_list.append(offset)\n",
    "    length_list.append(length)\n",
    "\n",
    "pk_num_df = pd.DataFrame({\"packet_number\": pk_num_list, \"offset\": offset_list, \"length\": length_list})\n",
    "pk_num_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_sent_rows = pd.concat([pk_sent_rows, pk_num_df], axis=1)\n",
    "pk_sent_rows = pk_sent_rows.reset_index(drop=True)\n",
    "\n",
    "pk_sent_rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_sent_rows['smoothed_rtt'] = np.nan\n",
    "pk_sent_rows['latest_rtt'] = np.nan\n",
    "pk_sent_rows['rtt_variance'] = np.nan\n",
    "pk_sent_rows['congestion_window'] = np.nan\n",
    "\n",
    "pk_sent_rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pk_sent_rows(row):\n",
    "    acked_ranges = row['acked_ranges']\n",
    "    smoothed_rtt = row['smoothed_rtt']\n",
    "    latest_rtt = row['latest_rtt']\n",
    "    rtt_variance = row['rtt_variance']\n",
    "    congestion_window = row['congestion_window']\n",
    "\n",
    "    for ack_range in acked_ranges:\n",
    "        start_packet, end_packet = ack_range[0], ack_range[-1]\n",
    "        existing_packets = set(pk_sent_rows['packet_number'])\n",
    "        packet_numbers_to_update = set(range(start_packet, end_packet + 1)).intersection(existing_packets)\n",
    "\n",
    "        mask = pk_sent_rows['packet_number'].isin(packet_numbers_to_update)\n",
    "        pk_sent_rows.loc[mask, 'smoothed_rtt'] = pk_sent_rows.loc[mask, 'smoothed_rtt'].fillna(smoothed_rtt)\n",
    "        pk_sent_rows.loc[mask, 'latest_rtt'] = pk_sent_rows.loc[mask, 'latest_rtt'].fillna(latest_rtt)\n",
    "        pk_sent_rows.loc[mask, 'congestion_window'] = pk_sent_rows.loc[mask, 'congestion_window'].fillna(congestion_window)\n",
    "        pk_sent_rows.loc[mask, 'rtt_variance'] = pk_sent_rows.loc[mask, 'rtt_variance'].fillna(rtt_variance)\n",
    "\n",
    "# Apply the custom update function to each row in rcv_ack_rows\n",
    "rcv_ack_rows.apply(update_pk_sent_rows, axis=1)\n",
    "\n",
    "# Display the updated pk_sent_rows\n",
    "pk_sent_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify lost packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ast.literal_eval to safely evaluate the string and extract 'packet_number'\n",
    "lost_rows['packet_number'] = lost_rows['data'].apply(lambda x: ast.literal_eval(x)['header']['packet_number'] if isinstance(x, str) else None)\n",
    "lost_rows['trigger'] = lost_rows['data'].apply(lambda x: ast.literal_eval(x)['trigger'] if isinstance(x, str) else None)\n",
    "lost_rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_pk_csv_file_path = path + \"middle/\" + f\"lost_pk_{time}_{port}.csv\"\n",
    "lost_rows.to_csv(lost_pk_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set to True if the packet is lost\n",
    "pk_sent_rows['packet_lost'] = False\n",
    "\n",
    "# Iterate through rows and set 'packet_lost' to True where 'packet_number' values match\n",
    "for _, lost_row in lost_rows.iterrows():\n",
    "    packet_number = lost_row['packet_number']\n",
    "    \n",
    "    # Check if 'packet_number' exists in pk_sent_rows\n",
    "    if packet_number in pk_sent_rows['packet_number'].values:\n",
    "        pk_sent_rows.loc[pk_sent_rows['packet_number'] == packet_number, 'packet_lost'] = True\n",
    "\n",
    "pk_sent_rows[19340:19345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['time', 'epoch_time', 'timestamp', 'name', 'packet_number', 'offset', 'length', 'bytes_in_flight', 'packets_in_flight', 'smoothed_rtt', 'latest_rtt', 'rtt_variance', 'congestion_window', 'packet_lost', 'data']\n",
    "processed_df = pk_sent_rows[cols]\n",
    "processed_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = path + \"data/\" + f\"processed_sent_{time}_{port}.csv\"\n",
    "processed_df.to_csv(csv_file_path, sep='@', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver side data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_rcv_df = pk_rcv_rows.reset_index(drop=True)\n",
    "pk_rcv_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_rcv_series =  pk_rcv_df['data']\n",
    "pk_rcv_num_list = []\n",
    "offset_rcv_list = []\n",
    "length_rcv_list = []\n",
    "for i in range(len(pk_rcv_series)):\n",
    "    s = pk_rcv_series.iloc[i]\n",
    "    data_dict = json.loads(s.replace(\"\\'\", \"\\\"\"))\n",
    "    packet_number = data_dict['header']['packet_number']\n",
    "    # Initialize offset to None in case 'frame_type': 'stream' is not found\n",
    "    offset = None\n",
    "    # Iterate through frames to find 'offset' for 'frame_type': 'stream'\n",
    "    for frame in data_dict.get('frames', []):\n",
    "        if frame.get('frame_type') == 'stream':\n",
    "            offset = frame.get('offset')\n",
    "            length = frame.get('length')\n",
    "            break  # Stop iterating once 'offset' is found\n",
    "    \n",
    "    pk_rcv_num_list.append(packet_number)\n",
    "    offset_rcv_list.append(offset)\n",
    "    length_rcv_list.append(length)\n",
    "\n",
    "pk_rcv_df['packet_number'] = pk_rcv_num_list\n",
    "pk_rcv_df['offset'] = offset_rcv_list\n",
    "pk_rcv_df['length'] = length_rcv_list\n",
    "\n",
    "pk_rcv_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['time', 'epoch_time', 'timestamp', 'name', 'packet_number', 'offset', 'length', 'data']\n",
    "processed_rcv_df = pk_rcv_df[cols]\n",
    "processed_rcv_df[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = path + \"data/\" + f\"processed_rcv_{time}_{port}.csv\"\n",
    "processed_rcv_df.to_csv(csv_file_path, sep='@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Packet Loss Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER SETTINGS ###\n",
    "database=\"/Volumes/mollyT7/MOXA/\"\n",
    "# database = \"/Volumes/MOLLY256/MOXA/\"\n",
    "# database = \"/Users/molly/Desktop\"\n",
    "dates = [\n",
    "    \"2024-03-13\",\n",
    "    # \"2024-03-21\",\n",
    "]\n",
    "devices = sorted([\n",
    "    \"sm00\",\n",
    "    # \"sm01\",\n",
    "])\n",
    "exps = {\n",
    "    # \"QUIC-450sec\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "    # \"QUIC-300sec\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "    \"QUIC-1M\": (2, [\"#{:02d}\".format(i + 1) for i in range(2)]),\n",
    "    \"QUIC-5M\": (2, [\"#{:02d}\".format(i + 1) for i in range(2)]),\n",
    "    \"QUIC-10M\": (2, [\"#{:02d}\".format(i + 1) for i in range(2)]),\n",
    "    # \"QUIC-inf\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "}\n",
    "\n",
    "device_to_port = {\"sm00\": [5200, 5201], \n",
    "                  \"sm01\": [5202, 5203],\n",
    "                  \"sm02\": [5204, 5205]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ul_sender_file(database, date, exp, device):\n",
    "    ul_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'raw')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if \"client.csv\" in file:\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[0]) in numbers:\n",
    "                        ul_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return ul_files\n",
    "\n",
    "def find_dl_sender_file(database, date, exp, device):\n",
    "    dl_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'raw')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if \"server.csv\" in file:\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[1]) in numbers:\n",
    "                        dl_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return dl_files\n",
    "\n",
    "def find_ul_rcv_file(database, date, exp, device):\n",
    "    ul_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'data')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if \"processed_rcv\" in file:\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[0]) in numbers:\n",
    "                        ul_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return ul_files\n",
    "\n",
    "def find_dl_rcv_file(database, date, exp, device):\n",
    "    dl_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'data')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if \"processed_rcv\" in file:\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[1]) in numbers:\n",
    "                        dl_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return dl_files\n",
    "\n",
    "def find_ul_loss_file(database, date, exp, device):\n",
    "    ul_loss_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'middle')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if \"lost_pk\" in file:\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[0]) in numbers:\n",
    "                        ul_loss_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return ul_loss_files\n",
    "\n",
    "def find_dl_loss_file(database, date, exp, device):\n",
    "    dl_loss_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'middle')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if \"lost_pk\" in file:\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[1]) in numbers:\n",
    "                        dl_loss_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return dl_loss_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_data(lost_df, received_df):\n",
    "    # Check if each row in ul_lost_df['packet_number'] is present in ul_received_df['packet_number']\n",
    "    lost_in_received = lost_df['packet_number'].isin(received_df['packet_number'])\n",
    "\n",
    "    # Get the rows in ul_lost_df where the packet number is present in ul_received_df\n",
    "    exec_lat_df = lost_df[lost_in_received]\n",
    "\n",
    "    exec_reorder_df = exec_lat_df[exec_lat_df['trigger'] == 'reordering_threshold']\n",
    "    exec_time_df = exec_lat_df[exec_lat_df['trigger'] == 'time_threshold']\n",
    "\n",
    "    # Get the rows in ul_lost_df where the packet number is not present in ul_received_df\n",
    "    real_lost_df = lost_df[~lost_in_received]\n",
    "\n",
    "    # Filter ul_lost_df for rows where 'trigger' is 'reordering threshold'\n",
    "    lost_reorder_df = real_lost_df[real_lost_df['trigger'] == 'reordering_threshold']\n",
    "    lost_time_df = real_lost_df[real_lost_df['trigger'] == 'time_threshold']\n",
    "\n",
    "    return exec_lat_df, exec_reorder_df, exec_time_df, real_lost_df, lost_reorder_df, lost_time_df\n",
    "\n",
    "def calculate_statistics(lost_reorder_df, lost_time_df, real_lost_df, exec_reorder_df, exec_time_df, exec_lat_df, lost_df, data_df, sent_df):\n",
    "    statistics_data = [{\n",
    "        'total_packets': len(sent_df),\n",
    "        'total_data_packets': len(data_df),\n",
    "        'original_pkl': len(lost_df),\n",
    "        'reordering_threshold': len(lost_reorder_df),\n",
    "        'time_threshold': len(lost_time_df),\n",
    "        'real_pkl': len(real_lost_df),\n",
    "        'exec_reordering': len(exec_reorder_df),\n",
    "        'exec_time': len(exec_time_df),\n",
    "        'exec_lat': len(exec_lat_df),\n",
    "        'reordering_pkl_rate(%)': 0 if len(real_lost_df) == 0 else len(lost_reorder_df)*100 / len(real_lost_df),\n",
    "        'time_pkl_rate(%)': 0 if len(real_lost_df) == 0 else len(lost_time_df)*100 / len(real_lost_df),\n",
    "        'real_pkl_rate(%)': 0 if len(lost_df) == 0 else len(real_lost_df)*100 / len(lost_df),\n",
    "        'original_packet_loss_rate(%)': len(lost_df)*100 / len(sent_df),\n",
    "        'adjusted_packet_loss_rate(%)': len(real_lost_df)*100 / len(sent_df)\n",
    "    }]\n",
    "\n",
    "    # Convert the dictionary to a dataframe\n",
    "    statistics_df = pd.DataFrame.from_dict(statistics_data)\n",
    "    return statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec_lat_df, exec_reorder_df, exec_time_df, real_lost_df, lost_reorder_df, lost_time_df = get_loss_data(lost_rows, processed_rcv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ul_sender_files = []\n",
    "all_ul_rcv_files = []\n",
    "all_ul_pkl_files = []\n",
    "for date in dates:\n",
    "    for exp in exps:\n",
    "        for device in devices:\n",
    "            ul_sender_files = find_ul_sender_file(database, date, exp, device)\n",
    "            ul_rcv_files = find_ul_rcv_file(database, date, exp, device)\n",
    "            ul_pk_loss_files = find_ul_loss_file(database, date, exp, device)\n",
    "            all_ul_sender_files.extend(ul_sender_files)\n",
    "            all_ul_rcv_files.extend(ul_rcv_files)\n",
    "            all_ul_pkl_files.extend(ul_pk_loss_files)\n",
    "\n",
    "for i in range(len(all_ul_rcv_files)):\n",
    "    ul_sender_df = pd.read_csv(all_ul_sender_files[i], sep=',')\n",
    "    # ul_sent_df is raw file, ul_rcv_df is processed file\n",
    "    ul_sent_df = ul_sender_df[(ul_sender_df['name'] == 'transport:packet_sent')]\n",
    "    ul_data_df = ul_sent_df[ul_sent_df['data'].str.contains(\"'frame_type': 'stream'\")]\n",
    "    ul_rcv_df = pd.read_csv(all_ul_rcv_files[i], sep='@')\n",
    "    # ul_rcver_df = pd.read_csv(all_ul_rcv_files[i], sep=',')\n",
    "    # ul_rcv_df = ul_rcver_df[ul_rcver_df['name'] == 'transport:packet_received']\n",
    "    # ul_rcv_df['packet_number'] = ul_rcv_df.apply(extract_packet_number, axis=1)\n",
    "    ul_loss_df = pd.read_csv(all_ul_pkl_files[i])\n",
    "    ul_exec_lat_df, ul_exec_reorder_df, ul_exec_time_df, ul_real_lost_df, ul_lost_reorder_df, ul_lost_time_df = get_loss_data(ul_loss_df, ul_rcv_df)\n",
    "    ul_statistics = calculate_statistics(ul_lost_reorder_df, ul_lost_time_df, ul_real_lost_df, ul_exec_reorder_df, ul_exec_time_df, ul_exec_lat_df, ul_loss_df, ul_data_df, ul_sent_df)\n",
    "\n",
    "    directory = os.path.dirname(all_ul_rcv_files[i])\n",
    "    ul_loss_df['lost'] = False\n",
    "    ul_loss_df['excl'] = False\n",
    "    # Set 'lost' column to True for rows in ul_real_lost_df\n",
    "    ul_loss_df.loc[ul_loss_df['packet_number'].isin(ul_real_lost_df['packet_number']), 'lost'] = True\n",
    "    # Set 'excl' column to True for rows in ul_exec_lat_df\n",
    "    ul_loss_df.loc[ul_loss_df['packet_number'].isin(ul_exec_lat_df['packet_number']), 'excl'] = True\n",
    "    parts = directory.split(\"/\")\n",
    "    parts[-1] = \"data\"\n",
    "    data_directory = \"/\".join(parts)\n",
    "    ul_loss_df.to_csv(f\"{data_directory}/ul_real_lost_pk.csv\", index=False)\n",
    "    # modify to the statistics directory\n",
    "    parts = directory.split(\"/\")\n",
    "    parts[-1] = \"statistics\"\n",
    "    statistics_directory = \"/\".join(parts)\n",
    "    ul_statistics.to_csv(f\"{statistics_directory}/ul_statistics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dl_sender_files = []\n",
    "all_dl_rcv_files = []\n",
    "all_dl_pkl_files = []\n",
    "for date in dates:\n",
    "    for exp in exps:\n",
    "        for device in devices:\n",
    "            dl_sender_files = find_dl_sender_file(database, date, exp, device)\n",
    "            dl_rcv_files = find_dl_rcv_file(database, date, exp, device)\n",
    "            dl_pk_loss_files = find_dl_loss_file(database, date, exp, device)\n",
    "            all_dl_sender_files.extend(dl_sender_files)\n",
    "            all_dl_rcv_files.extend(dl_rcv_files)\n",
    "            all_dl_pkl_files.extend(dl_pk_loss_files)\n",
    "            \n",
    "\n",
    "for i in range(len(all_dl_rcv_files)):\n",
    "    dl_sender_df = pd.read_csv(all_dl_sender_files[i], sep=',')\n",
    "    # ul_sent_df is raw file, ul_rcv_df is processed file\n",
    "    dl_sent_df = dl_sender_df[(dl_sender_df['name'] == 'transport:packet_sent')]\n",
    "    dl_data_df = dl_sent_df[dl_sent_df['data'].str.contains(\"'frame_type': 'stream'\")]\n",
    "    dl_rcv_df = pd.read_csv(all_dl_rcv_files[i], sep='@')\n",
    "    dl_loss_df = pd.read_csv(all_dl_pkl_files[i])\n",
    "    dl_exec_lat_df, dl_exec_reorder_df, dl_exec_time_df, dl_real_lost_df, dl_lost_reorder_df, dl_lost_time_df = get_loss_data(dl_loss_df, dl_rcv_df)\n",
    "    dl_statistics = calculate_statistics(dl_lost_reorder_df, dl_lost_time_df, dl_real_lost_df, dl_exec_reorder_df, dl_exec_time_df, dl_exec_lat_df, dl_loss_df, dl_data_df, dl_sent_df)\n",
    "    \n",
    "    directory = os.path.dirname(all_dl_rcv_files[i])\n",
    "    dl_loss_df['lost'] = False\n",
    "    dl_loss_df['excl'] = False\n",
    "    # Set 'lost' column to True for rows in dl_real_lost_df\n",
    "    dl_loss_df.loc[dl_loss_df['packet_number'].isin(dl_real_lost_df['packet_number']), 'lost'] = True\n",
    "    # Set 'excl' column to True for rows in ul_exec_lat_df\n",
    "    dl_loss_df.loc[dl_loss_df['packet_number'].isin(dl_exec_lat_df['packet_number']), 'excl'] = True\n",
    "    dl_loss_df.to_csv(f\"{directory}/dl_real_lost_pk.csv\", index=False)\n",
    "    # modify to the statistics directory\n",
    "    parts = directory.split(\"/\")\n",
    "    parts[-1] = \"statistics\"\n",
    "    statistics_directory = \"/\".join(parts)\n",
    "    dl_statistics.to_csv(f\"{statistics_directory}/dl_statistics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# def copy_file(source_file, destination_directory):\n",
    "#     try:\n",
    "#         shutil.copy(source_file, destination_directory)\n",
    "#         print(f\"File '{source_file}' copied successfully to '{destination_directory}'\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"File not found.\")\n",
    "#     except PermissionError:\n",
    "#         print(\"Permission denied.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date in dates:\n",
    "#     for exp in exps:\n",
    "#         for device in devices:\n",
    "#             exp_rounds, exp_list = exps[exp]\n",
    "#             ports = device_to_port.get(device, [])\n",
    "#             for exp_round in exp_list:\n",
    "#                 folder_path = os.path.join(database, date, exp, device, exp_round, 'statistics')\n",
    "#                 copy_file(f\"{folder_path}/ul_statistics.csv\", os.path.join(database, \"adjusted_stats\", date, exp, device, exp_round))\n",
    "#                 copy_file(f\"{folder_path}/dl_statistics.csv\", os.path.join(database, \"adjusted_stats\", date, exp, device, exp_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
