{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER SETTINGS ###\n",
    "database=\"/Volumes/mollyT7/MOXA\"\n",
    "# database = \"/Volumes/MOLLY256/MOXA/\"\n",
    "# database = \"/Users/molly/Desktop\"\n",
    "\n",
    "dates = [\n",
    "    \"2024-03-21\",\n",
    "    \"2024-05-08\",\n",
    "]\n",
    "devices = sorted([\n",
    "    \"sm00\",\n",
    "    \"sm01\",\n",
    "])\n",
    "exps = {\n",
    "    # \"QUIC-280sec\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "    # \"QUIC-300sec\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "    # \"QUIC-450sec\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "    # \"QUIC-1M\": (4, [\"#{:02d}\".format(i + 1) for i in range(4)]),\n",
    "    # \"QUIC-5M\": (4, [\"#{:02d}\".format(i + 1) for i in range(4)]),\n",
    "    # \"QUIC-10M\": (4, [\"#{:02d}\".format(i + 1) for i in range(4)]),\n",
    "    \"QUIC-inf\": (6, [\"#{:02d}\".format(i + 1) for i in range(6)]),\n",
    "}\n",
    "\n",
    "device_to_port = {\"sm00\": [5200, 5201], \n",
    "                  \"sm01\": [5202, 5203],\n",
    "                  \"sm02\": [5204, 5205]}\n",
    "\n",
    "figure_path = \"/Users/molly/Desktop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ordered_ho_datas = {}\n",
    "all_ho_dict = {'Conn_Rel': [], 'Conn_Req': [], 'LTE_HO': [], 'MN_HO': [], 'MN_HO_to_eNB': [], 'SN_setup': [], 'SN_Rel': [], 'SN_HO': [], \n",
    "               'RLF_II': [], 'RLF_III': [], 'SCG_RLF': []}\n",
    "event_dict = {'Conn_Rel': 1, 'Conn_Req': 2, 'LTE_HO': 3, 'MN_HO': 4, 'MN_HO_to_eNB': 5, 'SN_setup': 6, 'SN_Rel': 7, 'SN_HO': 8, 'stable': 0, \n",
    "              'RLF_II': 9, 'RLF_III': 10, 'SCG_RLF': 11, 'Add_Scell': 12}\n",
    "colors_dict = {'Conn_Rel': '#ed5555', 'Conn_Req': '#78c4b1', 'LTE_HO': '#3ea357', 'MN_HO': '#e8803a', \n",
    "               'MN_HO_to_eNB': '#ad58c4', 'SN_setup': '#ddbfde', 'SN_Rel': '#a1543f', 'SN_HO': '#d4c68a', 'stable': '#878483',\n",
    "               'RLF_II': '#59a2c2', 'RLF_III': '#6494c4', 'SCG_RLF': '#646fc4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stats_files(database, date, exp, device):\n",
    "    ul_files = []\n",
    "    dl_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'statistics')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.startswith(\"ul_statistics\"):\n",
    "                    ul_files.append(os.path.join(root, file))\n",
    "                if file.startswith(\"dl_statistics\"):\n",
    "                    dl_files.append(os.path.join(root, file))\n",
    "    return ul_files, dl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_stats(df):\n",
    "    avg_total_packets = int(df['total_packets'].mean())\n",
    "    avg_data_packets  = int(df['total_data_packets'].mean())\n",
    "    avg_original_pkl  = int(df['original_pkl'].mean())\n",
    "    avg_reordering_threshold = int(df['reordering_threshold'].mean())\n",
    "    avg_time_threshold = int(df['time_threshold'].mean())\n",
    "    avg_real_pkl = int(df['reordering_threshold'].mean()) + int(df['time_threshold'].mean())\n",
    "    \n",
    "    return {\"total_packets\": avg_total_packets,\n",
    "            \"total_data_packets\": avg_data_packets,\n",
    "            \"original_pkl\": avg_original_pkl,\n",
    "            \"reordering_threshold\": avg_reordering_threshold,\n",
    "            \"time_threshold\": avg_time_threshold,\n",
    "            \"real_pkl\": avg_real_pkl,\n",
    "            \"exec_reordering\": int(df['exec_reordering'].mean()),\n",
    "            \"exec_time\": int(df['exec_time'].mean()),\n",
    "            \"exec_lat\": int(df['exec_reordering'].mean()) + int(df['exec_time'].mean()),\n",
    "            \"reordering_pkl_rate(%)\": 0 if avg_real_pkl == 0 else avg_reordering_threshold*100 / avg_real_pkl,\n",
    "            \"time_pkl_rate(%)\": 0 if avg_real_pkl == 0 else avg_time_threshold*100 / avg_real_pkl,\n",
    "            \"real_pkl_rate(%)\": 0 if avg_original_pkl == 0 else avg_real_pkl*100 / avg_original_pkl,\n",
    "            \"original_packet_loss_rate(%)\": avg_original_pkl*100 / avg_total_packets,\n",
    "            \"adjusted_packet_loss_rate(%)\": avg_real_pkl*100 / avg_total_packets\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_ul_stats = {}\n",
    "all_avg_dl_stats = {}\n",
    "# Iterate over dates, exps, and devices\n",
    "for exp in exps:\n",
    "    exp_ul_stats_files = []\n",
    "    exp_dl_stats_files = []\n",
    "    for date in dates:\n",
    "        for device in devices:\n",
    "            ul_stats_files, dl_stats_files = find_stats_files(database, date, exp, device)\n",
    "            exp_ul_stats_files.extend(ul_stats_files)\n",
    "            exp_dl_stats_files.extend(dl_stats_files)\n",
    "    stats = []\n",
    "    # Iterate over each file path\n",
    "    for file_path in exp_ul_stats_files:\n",
    "        # Read CSV file into a DataFrame and append it to the list\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        stats.append(df)\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    exp_ul_stats = pd.concat(stats, ignore_index=True)\n",
    "\n",
    "    stats = []\n",
    "    for file_path in exp_dl_stats_files:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        stats.append(df)\n",
    "    exp_dl_stats = pd.concat(stats, ignore_index=True)\n",
    "\n",
    "    all_avg_ul_stats[exp] = calculate_avg_stats(exp_ul_stats)\n",
    "    all_avg_dl_stats[exp] = calculate_avg_stats(exp_dl_stats)\n",
    "\n",
    "all_avg_ul_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_dl_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_packet_loss_rate(all_avg_stats, ul_dl):\n",
    "    experiments = list(all_avg_stats.keys())\n",
    "    original_packet_loss_rates = [all_avg_stats[exp]['original_packet_loss_rate(%)'] for exp in experiments]\n",
    "    adjusted_packet_loss_rates = [all_avg_stats[exp]['adjusted_packet_loss_rate(%)'] for exp in experiments]\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the x-axis\n",
    "    r1 = range(len(experiments))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "\n",
    "    # Create the bars for original packet loss rate\n",
    "    plt.bar(r1, original_packet_loss_rates, color='b', width=bar_width, edgecolor='grey', label='Original Packet Loss Rate')\n",
    "    # Create the bars for adjusted packet loss rate\n",
    "    plt.bar(r2, adjusted_packet_loss_rates, color='r', width=bar_width, edgecolor='grey', label='Adjusted Packet Loss Rate')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Experiment', fontweight='bold')\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(experiments))], experiments, rotation=45)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.ylabel('Packet Loss Rate (%)', fontweight='bold')\n",
    "    plt.title(f'Packet Loss Rates for Different Bitrates({ul_dl})', fontweight='bold')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{figure_path}/{ul_dl}_avg_pkl_rate.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_packet_loss_rate(all_avg_ul_stats, \"ul\")\n",
    "plot_packet_loss_rate(all_avg_dl_stats, \"dl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_ratio(all_avg_stats, ul_dl):\n",
    "    experiments = list(all_avg_stats.keys())\n",
    "    reordering_pkl_rates = [all_avg_stats[exp]['reordering_pkl_rate(%)'] for exp in experiments]\n",
    "    time_pkl_rates = [all_avg_stats[exp]['time_pkl_rate(%)'] for exp in experiments]\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the x-axis\n",
    "    r1 = range(len(experiments))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "\n",
    "    # Create the bars for original packet loss rate\n",
    "    plt.bar(r1, reordering_pkl_rates, color='b', width=bar_width, edgecolor='grey', label='Reordering PKL Rate')\n",
    "\n",
    "    # Create the bars for adjusted packet loss rate\n",
    "    plt.bar(r2, time_pkl_rates, color='r', width=bar_width, edgecolor='grey', label='Time PKL Rate')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Experiment', fontweight='bold')\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(experiments))], experiments, rotation=45)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.ylabel('Packet Loss Rate (%)', fontweight='bold')\n",
    "    plt.title(f'Triggered Threshold Rates for Different Bitrates({ul_dl})', fontweight='bold')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{figure_path}/{ul_dl}_avg_threshold_pkl.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_threshold_ratio(all_avg_ul_stats, \"ul\")\n",
    "plot_threshold_ratio(all_avg_dl_stats, \"dl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_real_pkl_ratio(all_avg_stats, ul_dl):\n",
    "    experiments = list(all_avg_stats.keys())\n",
    "    reordering_pkl_rates = [all_avg_stats[exp]['reordering_pkl_rate(%)'] for exp in experiments]\n",
    "    time_pkl_rates = [all_avg_stats[exp]['time_pkl_rate(%)'] for exp in experiments]\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the x-axis\n",
    "    r1 = range(len(experiments))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "\n",
    "    # Create the bars for original packet loss rate\n",
    "    plt.bar(r1, reordering_pkl_rates, color='b', width=bar_width, edgecolor='grey', label='Reordering PKL Rate')\n",
    "\n",
    "    # Create the bars for adjusted packet loss rate\n",
    "    plt.bar(r2, time_pkl_rates, color='r', width=bar_width, edgecolor='grey', label='Time PKL Rate')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Experiment', fontweight='bold')\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(experiments))], experiments, rotation=45)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.ylabel('Packet Loss Rate (%)', fontweight='bold')\n",
    "    plt.title(f'Triggered Threshold Rates for Different Bitrates({ul_dl})', fontweight='bold')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{figure_path}/{ul_dl}_avg_real_pkl.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ul_sent_file(database, date, exp, device):\n",
    "    ul_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'data')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.startswith(\"processed_sent\"):\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[0]) in numbers:\n",
    "                        ul_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return ul_files\n",
    "\n",
    "def find_dl_sent_file(database, date, exp, device):\n",
    "    dl_files = []\n",
    "    exp_rounds, exp_list = exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'data')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.startswith(\"processed_sent\"):\n",
    "                    # Extract the numbers from the file name\n",
    "                    numbers = file.split(\"_\")[3]\n",
    "                    if str(ports[1]) in numbers:\n",
    "                        dl_files.append(os.path.join(root, file))\n",
    "                        break  # Exit the inner loop once the port is found\n",
    "    return dl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_goodput(sent_df):\n",
    "    df_copy = sent_df.copy()\n",
    "    df_copy.set_index(['packet_number', 'offset'], inplace=True)\n",
    "\n",
    "    # Group by 'offset' and count occurrences\n",
    "    offset_counts = df_copy.groupby(level='offset').size()\n",
    "\n",
    "    # Identify repeated and not repeated offsets\n",
    "    repeated_offsets = offset_counts[offset_counts > 1].index\n",
    "    not_repeated_offsets = offset_counts[offset_counts == 1].index\n",
    "\n",
    "    goodput = len(not_repeated_offsets)*100 / len(df_copy)\n",
    "    return goodput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_goodput(avg_ul_goodput, avg_dl_goodput):\n",
    "    # Extract experiment names and average goodput values\n",
    "    exp_names_ul = list(avg_ul_goodput.keys())\n",
    "    avg_ul_values = list(avg_ul_goodput.values())\n",
    "\n",
    "    exp_names_dl = list(avg_dl_goodput.keys())\n",
    "    avg_dl_values = list(avg_dl_goodput.values())\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot Uplink Goodput\n",
    "    plt.plot(exp_names_ul, avg_ul_values, marker='o', color='skyblue', label='Uplink Goodput')\n",
    "\n",
    "    # Plot Downlink Goodput\n",
    "    plt.plot(exp_names_dl, avg_dl_values, marker='o', color='lightgreen', label='Downlink Goodput')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Experiment')\n",
    "    plt.ylabel('Goodput (%)')\n",
    "    plt.title('Average Uplink and Downlink Goodput')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get all the files by different experiment\n",
    "# TODO: Calculate the average goodput\n",
    "# TODO: Draw the graph under differnet bitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ul_goodput = {}\n",
    "all_dl_goodput = {}\n",
    "# Iterate over dates, exps, and devices\n",
    "for exp in exps:\n",
    "    exp_ul_goodput = []\n",
    "    exp_dl_goodput = []\n",
    "    for date in dates:\n",
    "        for device in devices:\n",
    "            ul_sent_files = find_ul_sent_file(database, date, exp, device)\n",
    "            dl_sent_files = find_dl_sent_file(database, date, exp, device)\n",
    "            # print(ul_sent_files)\n",
    "            for ul_sent_file in ul_sent_files:\n",
    "                ul_sent_df = pd.read_csv(ul_sent_file, sep='@')\n",
    "                exp_ul_goodput.append(calulate_goodput(ul_sent_df))\n",
    "            for dl_sent_file in dl_sent_files:\n",
    "                dl_sent_df = pd.read_csv(dl_sent_file, sep='@')\n",
    "                exp_dl_goodput.append(calulate_goodput(dl_sent_df))\n",
    "\n",
    "    all_ul_goodput[exp] = exp_ul_goodput\n",
    "    all_dl_goodput[exp] = exp_dl_goodput\n",
    "\n",
    "avg_ul_goodput = {}\n",
    "avg_dl_goodput = {}\n",
    "# Calculate average uplink and downlink goodput for each experiment\n",
    "for exp, ul_goodput_list in all_ul_goodput.items():\n",
    "    avg_ul_goodput[exp] = sum(ul_goodput_list) / len(ul_goodput_list)\n",
    "for exp, dl_goodput_list in all_dl_goodput.items():\n",
    "    avg_dl_goodput[exp] = sum(dl_goodput_list) / len(dl_goodput_list)\n",
    "\n",
    "print(\"Average Uplink Goodput:\")\n",
    "print(avg_ul_goodput)\n",
    "print(\"\\nAverage Downlink Goodput:\")\n",
    "print(avg_dl_goodput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_goodput(avg_ul_goodput, avg_dl_goodput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handover Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lost_pk_file(database, date, exp, device):\n",
    "    ul_files = []\n",
    "    dl_files = []\n",
    "    rrc_files = []\n",
    "    exp_round, exp_list = exps[exp]\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'data')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            ul_file = \"\"\n",
    "            dl_file = \"\"\n",
    "            rrc_file = \"\"\n",
    "            for file in files:\n",
    "                if file.startswith(\"ul_real_lost_pk\"):\n",
    "                    ul_file = os.path.join(root, file)\n",
    "                if file.startswith(\"dl_real_lost_pk\"):\n",
    "                    dl_file = os.path.join(root, file)\n",
    "                if file.endswith(\"_rrc.csv\"):\n",
    "                    rrc_file = os.path.join(root, file)\n",
    "                    \n",
    "            if ul_file != \"\" and dl_file != \"\" and rrc_file != \"\":\n",
    "                ul_files.append(ul_file)\n",
    "                dl_files.append(dl_file)\n",
    "                rrc_files.append(rrc_file)\n",
    "\n",
    "    return ul_files, dl_files, rrc_files\n",
    "\n",
    "def get_lost_data_from_df(df):\n",
    "    lost_data_df = df[df['lost']==True]\n",
    "    return lost_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_records_within_1_second(loss_latency_df, D, event_dict):\n",
    "    # Initialize a dictionary to store the counts for each event type\n",
    "    event_counts = {}\n",
    "    # event = {'Event Type': [], 'HO Object': [], 'before': [], 'before_event': [], 'during':[], 'during_event': [], 'after': [], 'after_event': []}\n",
    "    events_list = []\n",
    "\n",
    "    # Loop through each event type in D\n",
    "    for event_type, ho_objects in D.items():\n",
    "        if event_type == \"Add_SCell\":\n",
    "            continue\n",
    "        event_counts[event_type] = {'before': [], 'during':[], 'after': []}\n",
    "\n",
    "        # Loop through each HO object in the current event type\n",
    "        for ho in ho_objects:\n",
    "            event = {'before': [], 'before_event': [], 'during':[], 'during_event': [], 'after': [], 'after_event': []}\n",
    "            if ho.end is not None:\n",
    "                start = ho.start\n",
    "                end = ho.end\n",
    "                sec_before_start = ho.start - timedelta(seconds=1)\n",
    "                sec_after_end = ho.end + timedelta(seconds=1)\n",
    "            else:\n",
    "                # continue\n",
    "                start = ho.start\n",
    "                end = ho.start\n",
    "                sec_before_start = ho.start - timedelta(seconds=1)\n",
    "                sec_after_end = ho.start + timedelta(seconds=1)\n",
    "            # print(start_time, ho.start, loss_latency_df['timestamp'])\n",
    "\n",
    "            # Filter and display the records in loss_latency_df within the specified time range\n",
    "            relevant_records_before = loss_latency_df[(start > loss_latency_df['timestamp']) & (loss_latency_df['timestamp'] >= sec_before_start)]\n",
    "            relevant_records_after = loss_latency_df[(end < loss_latency_df['timestamp']) & (loss_latency_df['timestamp'] <= sec_after_end)]\n",
    "            relevant_records_during = loss_latency_df[(start <= loss_latency_df['timestamp']) & (loss_latency_df['timestamp'] <= end)]\n",
    "\n",
    "            for records_df in [relevant_records_before, relevant_records_during, relevant_records_after]:\n",
    "                records_df.loc[records_df['ho_type'] == 0, 'ho_type'] = event_dict[event_type]\n",
    "\n",
    "            loss_latency_df.loc[relevant_records_before.index] = relevant_records_before\n",
    "            loss_latency_df.loc[relevant_records_during.index] = relevant_records_during\n",
    "            loss_latency_df.loc[relevant_records_after.index] = relevant_records_after\n",
    "            \n",
    "            # print(relevant_records_before['ho_type'])\n",
    "\n",
    "            event_counts[event_type]['before'].append(len(relevant_records_before))\n",
    "            # event_counts[event_type]['before_event'].append(relevant_records_before)\n",
    "            event_counts[event_type]['after'].append(len(relevant_records_after))\n",
    "            # event_counts[event_type]['after_event'].append(relevant_records_after)\n",
    "            event_counts[event_type]['during'].append(len(relevant_records_during))\n",
    "            # event_counts[event_type]['during_event'].append(relevant_records_during)\n",
    "\n",
    "            event['Event Type'] = f\"{event_type}\"\n",
    "            event['HO Object'] = f\"{ho}\"\n",
    "            event['before'] = len(relevant_records_before)\n",
    "            event['before_event'] = relevant_records_before\n",
    "            event['during'] = len(relevant_records_during)\n",
    "            event['during_event'] = relevant_records_during\n",
    "            event['after'] = len(relevant_records_after)\n",
    "            event['after_event'] = relevant_records_after\n",
    "\n",
    "            events_list.append(event)\n",
    "\n",
    "    return events_list, event_counts, loss_latency_df\n",
    "\n",
    "def calculate_event_sums_lists(event_counts):\n",
    "    success_sum_before = {}\n",
    "    success_sum_during = {}\n",
    "    success_sum_after = {}\n",
    "    failure_sum_before = {}\n",
    "    failure_sum_during= {}\n",
    "    failure_sum_after = {}\n",
    "    # Initialize lists to store the sums of before, during, and after counts\n",
    "    success_sum_before_list = []\n",
    "    success_sum_during_list = []\n",
    "    success_sum_after_list = []\n",
    "    failure_sum_before_list = []\n",
    "    failure_sum_during_list = []\n",
    "    failure_sum_after_list = []\n",
    "\n",
    "    # Loop through each event type\n",
    "    for event_type, counts in event_counts.items():\n",
    "        # Initialize sums for this event type\n",
    "        if (event_type == 'RLF_II') or (event_type == 'RLF_III') or (event_type == 'SCG_RLF'):\n",
    "            failure_sum_before[event_type] = sum(counts['before'])\n",
    "            failure_sum_during[event_type] = sum(counts['during'])\n",
    "            failure_sum_after[event_type] = sum(counts['after'])\n",
    "            failure_sum_before_list.append(sum(counts['before']))\n",
    "            failure_sum_during_list.append(sum(counts['during']))\n",
    "            failure_sum_after_list.append(sum(counts['after']))\n",
    "            \n",
    "        elif (event_type != 'Add_SCell'):\n",
    "            success_sum_before[event_type] = sum(counts['before'])\n",
    "            success_sum_during[event_type] = sum(counts['during'])\n",
    "            success_sum_after[event_type] = sum(counts['after'])\n",
    "            success_sum_before_list.append(sum(counts['before']))\n",
    "            success_sum_during_list.append(sum(counts['during']))\n",
    "            success_sum_after_list.append(sum(counts['after']))\n",
    "\n",
    "    return success_sum_before_list, success_sum_during_list, success_sum_after_list, failure_sum_before_list, failure_sum_during_list, failure_sum_after_list, success_sum_before, success_sum_during, success_sum_after, failure_sum_before, failure_sum_during, failure_sum_after\n",
    "\n",
    "def plot_event_sums(exp_name, success_sum_before_list, success_sum_during_list, success_sum_after_list, failure_sum_before_list, failure_sum_during_list, failure_sum_after_list, ul_dl):\n",
    "    x = ['Conn_Rel',\n",
    "         'Conn_Req',  # Setup\n",
    "         'LTE_HO',  # LTE -> newLTE\n",
    "         'MN_HO',  # LTE + NR -> newLTE + NR\n",
    "         'MN_HO_to_eNB',  # LTE + NR -> newLTE\n",
    "         'SN_setup',  # LTE -> LTE + NR => NR setup\n",
    "         'SN_Rel',  # LTE + NR -> LTE\n",
    "         'SN_HO',  # LTE + NR -> LTE + newNR\n",
    "         ]\n",
    "\n",
    "    w = 0.2\n",
    "\n",
    "    # Create an array of indices for x-axis positioning\n",
    "    indices = np.arange(len(x))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 5))  # 2 rows, 1 column of subplots\n",
    "    ax1.bar(indices, success_sum_before_list, color='#edc56d', width=0.15, align='edge', label='before event')\n",
    "    ax1.bar(indices + 1.1 * w, success_sum_during_list, color='#6db6ed', width=0.15, label='during event')\n",
    "    ax1.bar(indices + 1.8 * w, success_sum_after_list, color='#e4a2f2', width=0.15, label='after event')\n",
    "    if ul_dl == \"ul\":\n",
    "        ax1.set_title(f\"Handover Interval Loss: {exp_name} Uplink\")\n",
    "    elif ul_dl == \"dl\":\n",
    "        ax1.set_title(f\"Handover Interval Loss: {exp_name} Downlink\")\n",
    "    elif ul_dl == \"ul_real\":\n",
    "        ax1.set_title(f\"Handover Interval Loss: {exp_name} Uplink (Real)\")\n",
    "    elif ul_dl == \"dl_real\":\n",
    "        ax1.set_title(f\"Handover Interval Loss: {exp_name} Downlink (Real)\")\n",
    "    # Set custom x-axis labels\n",
    "    ax1.set_xticks(indices + w)\n",
    "    ax1.set_xticklabels(x)\n",
    "    ax1.legend()\n",
    "\n",
    "    x2 = ['RLF_II',  # fail but reestablishment success\n",
    "          'RLF_III',  # fail but reestablishment reject\n",
    "          'SCG_RLF'\n",
    "          ]\n",
    "\n",
    "    # Create an array of indices for x-axis positioning\n",
    "    indices2 = np.arange(len(x2))\n",
    "\n",
    "    ax2.bar(indices2, failure_sum_before_list, color='#edc56d', width=0.15, align='edge', label='before event')\n",
    "    ax2.bar(indices2 + 1.1 * w, failure_sum_during_list, color='#6db6ed', width=0.15, label='during event')\n",
    "    ax2.bar(indices2 + 1.8 * w, failure_sum_after_list, color='#e4a2f2', width=0.15, label='after event')\n",
    "    ax2.set_xticks(indices2 + 1 * w)\n",
    "    ax2.set_xticklabels(x2)\n",
    "    ax2.legend()\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{figure_path}/{exp_name}_ho_events_{ul_dl}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_files = {}\n",
    "# Iterate over dates, exps, and devices\n",
    "for exp in exps:\n",
    "    exp_data_files = {\"ul_lost_file\": [], \"dl_lost_file\": [], \"rrc_file\": []}\n",
    "    exp_ul_lost_pk_files = []\n",
    "    exp_dl_lost_pk_files = []\n",
    "    exp_rrc_files = []\n",
    "    for date in dates:\n",
    "        for device in devices:\n",
    "            # Find \"rrc\" files for the current combination of date, exp, and device\n",
    "            exp_ul_lost_pk_files, exp_dl_lost_pk_files, exp_rrc_files = find_lost_pk_file(database, date, exp, device)\n",
    "            exp_data_files[\"ul_lost_file\"].extend(exp_ul_lost_pk_files)\n",
    "            exp_data_files[\"dl_lost_file\"].extend(exp_dl_lost_pk_files)\n",
    "            exp_data_files[\"rrc_file\"].extend(exp_rrc_files)\n",
    "\n",
    "    all_data_files[exp] = exp_data_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_event_lost_sum_ul = {}\n",
    "all_event_lost_sum_dl = {}\n",
    "all_event_real_lost_sum_ul = {}\n",
    "all_event_real_lost_sum_dl = {}\n",
    "rrc_time_pattern = r\"(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})\"\n",
    "for exp in exps:\n",
    "    exp_event_lost_sum_ul = {\"success_before\": [0] * 8,\"success_during\": [0] * 8, \"success_after\": [0] * 8, \"failure_before\": [0] * 3, \"failure_during\": [0] * 3, \"failure_after\": [0] * 3}\n",
    "    exp_event_lost_sum_dl = {\"success_before\": [0] * 8,\"success_during\": [0] * 8, \"success_after\": [0] * 8, \"failure_before\": [0] * 3, \"failure_during\": [0] * 3, \"failure_after\": [0] * 3}\n",
    "    exp_event_real_lost_sum_ul = {\"success_before\": [0] * 8,\"success_during\": [0] * 8, \"success_after\": [0] * 8, \"failure_before\": [0] * 3, \"failure_during\": [0] * 3, \"failure_after\": [0] * 3}\n",
    "    exp_event_real_lost_sum_dl = {\"success_before\": [0] * 8,\"success_during\": [0] * 8, \"success_after\": [0] * 8, \"failure_before\": [0] * 3, \"failure_during\": [0] * 3, \"failure_after\": [0] * 3}\n",
    "    for idx, (ul_file, dl_file, rrc_file) in enumerate(zip(all_data_files[exp][\"ul_lost_file\"], all_data_files[exp][\"dl_lost_file\"], all_data_files[exp][\"rrc_file\"])):\n",
    "        # rrc_file\n",
    "        HOs = parse_mi_ho(rrc_file)\n",
    "        MRs = MeasureReport(rrc_file)\n",
    "        MRs = correct_MR_with_HO(MRs, HOs)\n",
    "        mappings = map_MR_HO(MRs, HOs)\n",
    "        ordered_HOs = print_trans(HOs, mappings=mappings)\n",
    "        match = re.search(rrc_time_pattern, os.path.basename(rrc_file))\n",
    "        if match:\n",
    "            start_time = match.group(1)\n",
    "            # Convert the timestamp string to a datetime object\n",
    "            start_timestamp = datetime.strptime(start_time, '%Y-%m-%d_%H-%M-%S')\n",
    "            # Filter entries based on the threshold timestamp\n",
    "            filtered_entries = [entry for entry in ordered_HOs if entry[1].start > start_timestamp]\n",
    "        start_timestamp = datetime.strptime(start_time, '%Y-%m-%d_%H-%M-%S')\n",
    "        # Filter the data based on the start timestamp\n",
    "        filtered_time_ordered_HO = [entry for entry in ordered_HOs if entry[1].start > start_timestamp]\n",
    "\n",
    "        # ul_sent_file\n",
    "        ul_lost_df = pd.read_csv(ul_file, encoding=\"utf-8\")\n",
    "        ul_lost_df['timestamp'] = pd.to_datetime(ul_lost_df['timestamp'])\n",
    "        ul_lost_df['ho_type'] = 0\n",
    "        event_list_ul, event_counts_ul, ul_lost_df = count_records_within_1_second(ul_lost_df, HOs, event_dict)\n",
    "        success_sum_before_list_ul, success_sum_during_list_ul, success_sum_after_list_ul, failure_sum_before_list_ul, failure_sum_during_list_ul, failure_sum_after_list_ul, success_sum_before_ul, success_sum_during_ul, success_sum_after_ul, failure_sum_before_ul, failure_sum_during_ul, failure_sum_after_ul = calculate_event_sums_lists(event_counts_ul)\n",
    "        ul_real_lost_df = ul_lost_df[ul_lost_df['lost']==True]\n",
    "        event_list_ul_real, event_counts_ul_real, ul_real_lost_df = count_records_within_1_second(ul_real_lost_df, HOs, event_dict)\n",
    "        success_sum_before_list_ul_real, success_sum_during_list_ul_real, success_sum_after_list_ul_real, failure_sum_before_list_ul_real, failure_sum_during_list_ul_real, failure_sum_after_list_ul_real, success_sum_before_ul_real, success_sum_during_ul_real, success_sum_after_ul_real, failure_sum_before_ul_real, failure_sum_during_ul_real, failure_sum_after_ul_real = calculate_event_sums_lists(event_counts_ul_real)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Success Sum Before List (ul):\", success_sum_before_list_ul)\n",
    "        print(\"Success Sum During List (ul):\", success_sum_during_list_ul)\n",
    "        print(\"Success Sum After List (ul):\", success_sum_after_list_ul)\n",
    "        print(\"Failure Sum Before List (ul):\", failure_sum_before_list_ul)\n",
    "        print(\"Failure Sum During List (ul):\", failure_sum_during_list_ul)\n",
    "        print(\"Failure Sum After List (ul):\", failure_sum_after_list_ul)\n",
    "\n",
    "        print(\"Success Sum Before List (ul_real):\", success_sum_before_list_ul_real)\n",
    "        print(\"Success Sum During List (ul_real):\", success_sum_during_list_ul_real)\n",
    "        print(\"Success Sum After List (ul_real):\", success_sum_after_list_ul_real)\n",
    "        print(\"Failure Sum Before List (ul_real):\", failure_sum_before_list_ul_real)\n",
    "        print(\"Failure Sum During List (ul_real):\", failure_sum_during_list_ul_real)\n",
    "        print(\"Failure Sum After List (ul_real):\", failure_sum_after_list_ul_real)\n",
    "\n",
    "\n",
    "        exp_event_lost_sum_ul = {\"success_before\": [a + b for a, b in zip(exp_event_lost_sum_ul[\"success_before\"], success_sum_before_list_ul)], \n",
    "                                 \"success_during\": [a + b for a, b in zip(exp_event_lost_sum_ul[\"success_during\"], success_sum_during_list_ul)], \n",
    "                                 \"success_after\": [a + b for a, b in zip(exp_event_lost_sum_ul[\"success_after\"], success_sum_after_list_ul)], \n",
    "                                 \"failure_before\": [a + b for a, b in zip(exp_event_lost_sum_ul[\"failure_before\"], failure_sum_before_list_ul)], \n",
    "                                 \"failure_during\": [a + b for a, b in zip(exp_event_lost_sum_ul[\"failure_during\"], failure_sum_during_list_ul)], \n",
    "                                 \"failure_after\": [a + b for a, b in zip(exp_event_lost_sum_ul[\"failure_after\"], failure_sum_after_list_ul)]\n",
    "                                }\n",
    "        exp_event_real_lost_sum_ul = {\"success_before\": [a + b for a, b in zip(exp_event_real_lost_sum_ul[\"success_before\"], success_sum_before_list_ul_real)], \n",
    "                                      \"success_during\": [a + b for a, b in zip(exp_event_real_lost_sum_ul[\"success_during\"], success_sum_during_list_ul_real)], \n",
    "                                      \"success_after\": [a + b for a, b in zip(exp_event_real_lost_sum_ul[\"success_after\"], success_sum_after_list_ul_real)], \n",
    "                                      \"failure_before\": [a + b for a, b in zip(exp_event_real_lost_sum_ul[\"failure_before\"], failure_sum_before_list_ul_real)], \n",
    "                                      \"failure_during\": [a + b for a, b in zip(exp_event_real_lost_sum_ul[\"failure_during\"], failure_sum_during_list_ul_real)], \n",
    "                                      \"failure_after\": [a + b for a, b in zip(exp_event_real_lost_sum_ul[\"failure_after\"], failure_sum_after_list_ul_real)]\n",
    "                                     }\n",
    "\n",
    "        # dl_sent_file\n",
    "        dl_lost_df = pd.read_csv(dl_file, encoding=\"utf-8\")\n",
    "        dl_lost_df['timestamp'] = pd.to_datetime(dl_lost_df['timestamp'])\n",
    "        dl_lost_df['ho_type'] = 0\n",
    "        event_list_dl, event_counts_dl, dl_lost_df = count_records_within_1_second(dl_lost_df, HOs, event_dict)\n",
    "        success_sum_before_list_dl, success_sum_during_list_dl, success_sum_after_list_dl, failure_sum_before_list_dl, failure_sum_during_list_dl, failure_sum_after_list_dl, success_sum_before_dl, success_sum_during_dl, success_sum_after_dl, failure_sum_before_dl, failure_sum_during_dl, failure_sum_after_dl = calculate_event_sums_lists(event_counts_dl)\n",
    "        dl_real_lost_df = dl_lost_df[dl_lost_df['lost']==True]\n",
    "        event_list_dl_real, event_counts_dl_real, dl_real_lost_df = count_records_within_1_second(dl_real_lost_df, HOs, event_dict)\n",
    "        success_sum_before_list_dl_real, success_sum_during_list_dl_real, success_sum_after_list_dl_real, failure_sum_before_list_dl_real, failure_sum_during_list_dl_real, failure_sum_after_list_dl_real, success_sum_before_dl_real, success_sum_during_dl_real, success_sum_after_dl_real, failure_sum_before_dl_real, failure_sum_during_dl_real, failure_sum_after_dl_real = calculate_event_sums_lists(event_counts_dl_real)\n",
    "\n",
    "        \n",
    "        # Print the results\n",
    "        print(\"Success Sum Before List (dl):\", success_sum_before_list_dl)\n",
    "        print(\"Success Sum During List (dl):\", success_sum_during_list_dl)\n",
    "        print(\"Success Sum After List (dl):\", success_sum_after_list_dl)\n",
    "        print(\"Failure Sum Before List (dl):\", failure_sum_before_list_dl)\n",
    "        print(\"Failure Sum During List (dl):\", failure_sum_during_list_dl)\n",
    "        print(\"Failure Sum After List (dl):\", failure_sum_after_list_dl)\n",
    "\n",
    "        print(\"Success Sum Before List (dl_real):\", success_sum_before_list_dl_real)\n",
    "        print(\"Success Sum During List (dl_real):\", success_sum_during_list_dl_real)\n",
    "        print(\"Success Sum After List (dl_real):\", success_sum_after_list_dl_real)\n",
    "        print(\"Failure Sum Before List (dl_real):\", failure_sum_before_list_dl_real)\n",
    "        print(\"Failure Sum During List (dl_real:\", failure_sum_during_list_dl_real)\n",
    "        print(\"Failure Sum After List (dl_real):\", failure_sum_after_list_dl_real)\n",
    "\n",
    "        exp_event_lost_sum_dl = {\"success_before\": [a + b for a, b in zip(exp_event_lost_sum_dl[\"success_before\"], success_sum_before_list_dl)], \n",
    "                                 \"success_during\": [a + b for a, b in zip(exp_event_lost_sum_dl[\"success_during\"], success_sum_during_list_dl)], \n",
    "                                 \"success_after\": [a + b for a, b in zip(exp_event_lost_sum_dl[\"success_after\"], success_sum_after_list_dl)], \n",
    "                                 \"failure_before\": [a + b for a, b in zip(exp_event_lost_sum_dl[\"failure_before\"], failure_sum_before_list_dl)], \n",
    "                                 \"failure_during\": [a + b for a, b in zip(exp_event_lost_sum_dl[\"failure_during\"], failure_sum_during_list_dl)],\n",
    "                                 \"failure_after\": [a + b for a, b in zip(exp_event_lost_sum_dl[\"failure_after\"], failure_sum_after_list_dl)]\n",
    "                                }\n",
    "        \n",
    "        exp_event_real_lost_sum_dl = {\"success_before\": [a + b for a, b in zip(exp_event_real_lost_sum_dl[\"success_before\"], success_sum_before_list_dl_real)], \n",
    "                                      \"success_during\": [a + b for a, b in zip(exp_event_real_lost_sum_dl[\"success_during\"], success_sum_during_list_dl_real)], \n",
    "                                      \"success_after\": [a + b for a, b in zip(exp_event_real_lost_sum_dl[\"success_after\"], success_sum_after_list_dl_real)], \n",
    "                                      \"failure_before\": [a + b for a, b in zip(exp_event_real_lost_sum_dl[\"failure_before\"], failure_sum_before_list_dl_real)], \n",
    "                                      \"failure_during\": [a + b for a, b in zip(exp_event_real_lost_sum_dl[\"failure_during\"], failure_sum_during_list_dl_real)],\n",
    "                                      \"failure_after\": [a + b for a, b in zip(exp_event_real_lost_sum_dl[\"failure_after\"], failure_sum_after_list_dl_real)]\n",
    "                                     }\n",
    "    \n",
    "    all_event_lost_sum_ul[exp] = exp_event_lost_sum_ul\n",
    "    all_event_lost_sum_dl[exp] = exp_event_lost_sum_dl\n",
    "    all_event_real_lost_sum_ul[exp] = exp_event_real_lost_sum_ul\n",
    "    all_event_real_lost_sum_dl[exp] = exp_event_real_lost_sum_dl\n",
    "\n",
    "    plot_event_sums(exp, exp_event_lost_sum_ul[\"success_before\"], exp_event_lost_sum_ul[\"success_during\"], exp_event_lost_sum_ul[\"success_after\"], exp_event_lost_sum_ul[\"failure_before\"], exp_event_lost_sum_ul[\"failure_during\"], exp_event_lost_sum_ul[\"failure_after\"], \"ul\")\n",
    "    plot_event_sums(exp, exp_event_lost_sum_dl[\"success_before\"], exp_event_lost_sum_dl[\"success_during\"], exp_event_lost_sum_dl[\"success_after\"], exp_event_lost_sum_dl[\"failure_before\"], exp_event_lost_sum_dl[\"failure_during\"], exp_event_lost_sum_dl[\"failure_after\"], \"dl\")\n",
    "    plot_event_sums(exp, exp_event_real_lost_sum_ul[\"success_before\"], exp_event_real_lost_sum_ul[\"success_during\"], exp_event_real_lost_sum_ul[\"success_after\"], exp_event_real_lost_sum_ul[\"failure_before\"], exp_event_real_lost_sum_ul[\"failure_during\"], exp_event_real_lost_sum_ul[\"failure_after\"], \"ul_real\")\n",
    "    plot_event_sums(exp, exp_event_real_lost_sum_dl[\"success_before\"], exp_event_real_lost_sum_dl[\"success_during\"], exp_event_real_lost_sum_dl[\"success_after\"], exp_event_real_lost_sum_dl[\"failure_before\"], exp_event_real_lost_sum_dl[\"failure_during\"], exp_event_real_lost_sum_dl[\"failure_after\"], \"dl_real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER SETTINGS ###\n",
    "database=\"/Volumes/mollyT7/MOXA\"\n",
    "# database = \"/Volumes/MOLLY256/MOXA/\"\n",
    "# database = \"/Users/molly/Desktop\"\n",
    "\n",
    "dates = [\n",
    "    # \"/Users/molly/Desktop/2024-01-26\",\n",
    "    # \"2024-02-03\",\n",
    "    \"2024-03-13\",\n",
    "    # \"2024-03-21\",\n",
    "]\n",
    "devices = sorted([\n",
    "    \"sm00\",\n",
    "    \"sm01\",\n",
    "])\n",
    "udp_exps = {\n",
    "    \"UDP-1M\": (2, [\"#{:02d}\".format(i + 1) for i in range(2)]),\n",
    "    \"UDP-5M\": (2, [\"#{:02d}\".format(i + 1) for i in range(2)]),\n",
    "    \"UDP-10M\": (2, [\"#{:02d}\".format(i + 1) for i in range(2)]),\n",
    "}\n",
    "\n",
    "device_to_port = {\"sm00\": [3200, 3201], \n",
    "                  \"sm01\": [3202, 3203],\n",
    "                  \"sm02\": [3204, 3205]}\n",
    "\n",
    "figure_path = \"/Users/molly/Desktop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_loss_stats_files(database, date, exp, device):\n",
    "    ul_files = []\n",
    "    dl_files = []\n",
    "    exp_rounds, exp_list = udp_exps[exp]\n",
    "    ports = device_to_port.get(device, [])\n",
    "    for exp_round in exp_list:\n",
    "        folder_path = os.path.join(database, date, exp, device, exp_round, 'statistics')\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.startswith(\"udp_uplk_loss_statistics\"):\n",
    "                    ul_files.append(os.path.join(root, file))\n",
    "                if file.startswith(\"udp_dnlk_loss_statistics\"):\n",
    "                    dl_files.append(os.path.join(root, file))\n",
    "    return ul_files, dl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_udp_stats(df):\n",
    "    avg_total_packets = int(df['total_packet_sent'].mean())\n",
    "    avg_packet_loss = int(df['total_packet_loss'].mean())\n",
    "    \n",
    "    return {\"total_packet_sent\": avg_total_packets,\n",
    "            \"total_packet_loss\": avg_packet_loss,\n",
    "            \"packet_loss_rate(%)\": avg_packet_loss*100 / avg_total_packets,\n",
    "            \"experiment_time(sec)\": df['experiment_time(sec)'].mean()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_avg_udp_ul_stats = {}\n",
    "all_avg_udp_dl_stats = {}\n",
    "# Iterate over dates, exps, and devices\n",
    "for exp in udp_exps:\n",
    "    exp_udp_ul_stats_files = []\n",
    "    exp_udp_dl_stats_files = []\n",
    "    for date in dates:\n",
    "        for device in devices:\n",
    "            ul_stats_files, dl_stats_files = find_loss_stats_files(database, date, exp, device)\n",
    "            exp_udp_ul_stats_files.extend(ul_stats_files)\n",
    "            exp_udp_dl_stats_files.extend(dl_stats_files)\n",
    "    stats = []\n",
    "    # Iterate over each file path\n",
    "    for file_path in exp_udp_ul_stats_files:\n",
    "        # Read CSV file into a DataFrame and append it to the list\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        stats.append(df)\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    exp_ul_stats = pd.concat(stats, ignore_index=True)\n",
    "\n",
    "    stats = []\n",
    "    for file_path in exp_udp_dl_stats_files:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "        stats.append(df)\n",
    "    exp_dl_stats = pd.concat(stats, ignore_index=True)\n",
    "\n",
    "    all_avg_udp_ul_stats[exp] = calculate_avg_udp_stats(exp_ul_stats)\n",
    "    all_avg_udp_dl_stats[exp] = calculate_avg_udp_stats(exp_dl_stats)\n",
    "\n",
    "all_avg_udp_ul_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_udp_dl_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_udp_packet_loss_rate(all_avg_stats, ul_dl):\n",
    "    experiments = list(all_avg_stats.keys())\n",
    "    packet_loss_rate = [all_avg_stats[exp]['packet_loss_rate(%)'] for exp in experiments]\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the x-axis\n",
    "    r1 = range(len(experiments))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "\n",
    "    # Create the bars for original packet loss rate\n",
    "    plt.bar(r1, packet_loss_rate, color='b', width=bar_width, edgecolor='grey', label='Packet Loss Rate')\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    plt.xlabel('Experiment', fontweight='bold')\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(experiments))], experiments, rotation=45)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.ylabel('Packet Loss Rate (%)', fontweight='bold')\n",
    "    plt.title('Packet Loss Rates for Different Bitrates', fontweight='bold')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', color='gray', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{figure_path}/{ul_dl}_avg_udp_pkl_rate.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_udp_packet_loss_rate(all_avg_udp_ul_stats, \"ul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_udp_packet_loss_rate(all_avg_udp_dl_stats, \"dl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def QlogToJsonEntry(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Add commas between lines\n",
    "    json_str = \",\".join(lines)\n",
    "    # Surround the entire string with square brackets to create a JSON array\n",
    "    json_str = \"[\" + json_str + \"]\"\n",
    "    # Load the JSON array\n",
    "    json_entry = json.loads(json_str)\n",
    "    \n",
    "    return json_entry\n",
    "\n",
    "def QlogToJson(json_entry, json_file_path):\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(json_entry, json_file, indent=2)\n",
    "\n",
    "def JsonToCsv(json_entry, csv_file_path):\n",
    "     # Open CSV file for writing\n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        # Create a CSV writer\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write header row based on the keys of the second JSON object (assuming at least two objects are present)\n",
    "        if len(json_entry) >= 2:\n",
    "            header = list(json_entry[1].keys())\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "            # Write data rows starting from the second object\n",
    "            for entry in json_entry[1:]:\n",
    "                csv_writer.writerow(entry.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender_side_file\n",
    "sent_raw_path = \"/Volumes/mollyT7/MOXA/2024-04-24/QUIC-10M/sm01/#02/raw/log_20240424_1347_5203_server\"\n",
    "sent_qlog_file_path = sent_raw_path + \".qlog\"\n",
    "sent_json_file_path = sent_raw_path + \".json\"\n",
    "sent_csv_file_path = sent_raw_path + \".csv\"\n",
    "sent_json_entry = QlogToJsonEntry(sent_qlog_file_path)\n",
    "# QlogToJson(sent_json_entry, sent_json_file_path)\n",
    "JsonToCsv(sent_json_entry, sent_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_sender_df = pd.read_csv(sent_csv_file_path)\n",
    "dl_sent_df = dl_sender_df[(dl_sender_df['name'] == 'transport:packet_sent')]\n",
    "dl_lost_df = dl_sender_df[(dl_sender_df['name'] == 'recovery:packet_lost')]\n",
    "dl_received_df = dl_sender_df[(dl_sender_df['name'] == 'transport:packet_received')]\n",
    "pk_sent_rows = dl_sent_df['data'].str.contains(\"'frame_type': 'stream'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dl_sent_df), len(dl_lost_df), len(dl_received_df))\n",
    "print(len(dl_sent_df) + len(dl_received_df))\n",
    "print(len(pk_sent_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
